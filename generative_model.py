import torch
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import os
import cv2

# logsigma, tanhmurot, etc.
# loga?

def gauss(mu, sigma, Z, cutoff=True):
    def f(x):
        k = ((x - mu) / sigma)**2
        g = torch.exp(-0.5 * k) / Z

        if cutoff:
            return torch.where(g >= np.exp(-3.) / Z, g, torch.zeros_like(g))
        return g

    return f

def gauss2d(mu_X, mu_Y, sigma_X, sigma_Y, rot, Z, cutoff=True):
    Inverse_Lambda = torch.zeros([2, 2], dtype=torch.float32, device=mu_X.device)
    Inverse_Lambda[0, 0] = sigma_X**(-2)
    Inverse_Lambda[1, 1] = sigma_Y**(-2)

    U = torch.zeros([2, 2], dtype=torch.float32, device=mu_X.device)
    U[0, 0] = torch.cos(rot)
    U[0, 1] = torch.sin(rot)
    U[1, 0] = -torch.sin(rot)
    U[1, 1] = torch.cos(rot)

    Inverse_Sigma = torch.matmul(U.T, torch.matmul(Inverse_Lambda, U))

    mu = torch.zeros([2, 1], dtype=torch.float32, device=mu_X.device)
    mu[0, 0] = mu_X
    mu[1, 0] = mu_Y

    def f(x, y):
        X = torch.stack([x, y], 1).unsqueeze(2)

        k = torch.matmul(
            torch.transpose((X - mu.unsqueeze(0)), 2, 1),
            torch.matmul(Inverse_Sigma.unsqueeze(0),
                (X - mu.unsqueeze(0))))[:, 0, 0]
        g = torch.exp(-0.5 * k) / Z

        if cutoff:
            return torch.where(g >= np.exp(-3.) / Z, g, torch.zeros_like(g))
        return g

    return f

def generate_video(
    phis,
    alpha_T, alpha_Y, alpha_X,
    stride_T, stride_Y, stride_X,
    a,
    mu_T, mu_Y, mu_X,
    sigma_T, sigma_Y, sigma_X,
    rot,
    plot_save_dir=None, RGB=False,
):
    """
    The model:
        A video is generated by the weighted sum of (n_philters) basis functions (also known as "kernels"
            and here referred to as "philters").
            The philters are 4-D and span the time, spatial Y, spatial X, and channel dimensions.
            In this implementation, each philter is restricted to explaining a small chunk of the video
                which is localized in the [time(t), spatial Y(n_y), spatial X(n_x), channel(ch)] dimensions.
                The full video is reconstructed by tiling a weighted sum of these philters across the
                time, spatial, and channel dimensions. The spacing of the tiling is controlled by "stride"
                terms, while the upsampling factor from the philters to the video is controlled by "alpha" 
                terms. If there are [T, N_y, N_x, CH] philters in the tiling, then the output video will
                have dimensions of approximately [T*t*alpha_T*stride_T, N_y*y*alpha_Y*stride_Y, 
                    N_x*x*alpha_X*stride_X, CH*ch].
            There may be a different philter sets for each spatial location and/or for each channel, or
                the basis functions (philter sets) can be shared across the spatial and/or channel dimensions.
                If the philter sets differ across spatial locations, there must be [N_y, N_x] of them.
                Similarly, if the philter sets differ across channels, there must be [CH] of them.
        Additionally, the philters can be scaled or shifted locally to accomodate for local deformations 
            to the philters/basis functions (this can be interpreted as performing dynamic routing).
            Deformations (shifting, scaling, and rotation) and upsampling are performed for the spatial 
                dimensions before the time dimension.
            This implementation does not support deformations along the channel (color) dimension.
                (Reasoning: Typically, the reflectance spectrum (color) of an object does
                not change over time or space. The illumination (shading) may change, but 
                this can already be modelled by changes in the philter coefficients rather 
                than deformations in the philters themselves.)
        A sparse prior is used for the philter coefficients and for the deformation coefficients.
        The model can be extended to incorporate hierarchical structure over the philter coefficients
            and the deformation coefficients. This may result in a more efficient, distributed coding
            for the video. Additionally, spatial and temporal structure may be captured at multiple
            scales, and certain deformations of an object's perspective projection can be modelled 
            as being more likely than others (e.g., a rotation vs. an inversion).

    #######################################################################

    INPUTS:
        PHILTERS (filter bank)
            phis: [[N_y, N_x], [CH], n_philters, t, n_y, n_x, ch]
                The last 4 dimensions (t, n_y, n_x, ch) define a philter.
                Indexing into phis to select a philter occurs as follows:
                TIME DIMENSION (no indexing):
                    Philters are shared across the time dimension (they do not change per-timestep,
                        so as to handle videos of arbitrary length and to reflect constant, continuous,
                        real-time processing), but they can be shifted & scaled along the time dimension.
                SPATIAL DIMENSION (N_y, N_x):
                    Philters can vary across spatial locations, but they may also be shared.
                    If philters are shared across space, this is referred to as the "convolutional" case.
                    Convolutional philters are specified by N_y = N_x = 1, 
                        or by omitting those dimensions from phis.
                CHANNEL DIMENSION (CH):
                    Philters may vary across the channel dimension, but they may also be shared.
                    If philters vary across the channel dimension, 
                        this is known as "depthwise", "separable", or "grouped" convolution.
                    This is specified by including the CH dimension in phis, or CH =/= 1.
        ALPHAS (upsampling factors)
            alpha_T:
                Upsampling in the time dimension.
                Applied AFTER spatial dimension transformations.
            alpha_Y:
                Upsampling in the spatial Y dimension.
            alpha_X:
                Upsampling in the spatial X dimension.
            NOTES:
                Fractional upsampling is allowed.
        STRIDES (philter spacing)
            stride_T:
                Philter spacing in the time dimension, in units of t*alpha_T.
            stride_Y:
                Philter spacing in the spatial Y dimension, in units of n_y*alpha_Y.
            stride_X:
                Philter spacing in the spatial X dimension, in units of n_x*alpha_X.
            NOTES:
                Fractional strides are allowed.
                A stride < 1 indicates overlap between the philters. 
                Strides > 1 are not recommended, since there will be empty space between the philters.
        A (philter coefficients)
            a: [batch, T, N_y, N_x, [CH], n_philters]
                Philter coefficients.
        MUS (philter shifts)
            mu_T: [batch, T, N_y, N_x, [CH], n_philters]
                Philter shifts in the time dimension, in units of t*alpha_T.
                Applied AFTER spatial dimension transformations.
            mu_Y: [batch, T, N_y, N_x, [CH], n_philters]
                Philter shifts in the spatial Y dimension, in units of n_y*alpha_Y.
            mu_X: [batch, T, N_y, N_x, [CH], n_philters]
                Philter shifts in the spatial X dimension, in units of n_x*alpha_X.
        SIGMAS (philter scaling)
            sigma_T: [batch, T, N_y, N_x, [CH], n_philters]
                Philter scaling in the time dimension, relative to alpha_T.
                Applied AFTER spatial dimension transformations.
            sigma_Y: [batch, T, N_y, N_x, [CH], n_philters]
                Philter scaling in the spatial Y dimension, relative to alpha_Y.
            sigma_X: [batch, T, N_y, N_x, [CH], n_philters]
                Philter scaling in the spatial X dimension, relative to alpha_X.
        ROT (philter rotations)
            rot: [batch, T, N_y, N_x, [CH], n_philters]
                Philter rotation in the spatial XY plane.
            NOTES:
                The rotation should be bounded by +/-(pi/4).
        PLOT (plotting settings)
            plot_save_dir:
                Directory to save the plots to.
            RGB:
                Whether or not to combine the first 3 channels into an RGB image instead of
                    plotting each channel separately.
    
    OUTPUTS:
        A video of dimension [batch, ~T*t*alpha_T*stride_T, ~N_y*y*alpha_Y*stride_Y, 
            ~N_x*x*alpha_X*stride_X, CH*ch]. (Approximate values due to rounding & stride.)
        Optionally, saves plots for the generated videos to plot_save_dir.
    """

    ### DIMENSION MUNGING ###
    if len(a.shape) == 5:
        a = a.unsqueeze(4)
        mu_T = mu_T.unsqueeze(4)
        mu_Y = mu_Y.unsqueeze(4)
        mu_X = mu_X.unsqueeze(4)
        sigma_T = sigma_T.unsqueeze(4)
        sigma_Y = sigma_Y.unsqueeze(4)
        sigma_X = sigma_X.unsqueeze(4)
        rot = rot.unsqueeze(4)
    batch_size, T, N_y, N_x, CH, _ = a.shape

    if len(phis.shape) == 5:
        phis = phis.unsqueeze(0).unsqueeze(1).unsqueeze(2)
    if len(phis.shape) == 6:
        phis = phis.unsqueeze(0).unsqueeze(1)
    if len(phis.shape) == 7:
        phis = phis.unsqueeze(2)
    if phis.shape[0] == 1 and N_y > 1:
        phis = phis.repeat(N_y, 1, 1, 1, 1, 1, 1, 1)
    if phis.shape[1] == 1 and N_x > 1:
        phis = phis.repeat(1, N_x, 1, 1, 1, 1, 1, 1)
    if phis.shape[2] == 1 and CH > 1:
        phis = phis.repeat(1, 1, CH, 1, 1, 1, 1, 1)
    _, _, _, n_philters, t, n_y, n_x, ch = phis.shape

    assert a.shape[5] == n_philters, \
        "a suggests n_philters is {} but phis suggests {}".format(a.shape[5], n_philters)

    ### GENERATE VIDEO ###
    T_max = int(np.ceil(t*alpha_T * (stride_T*(T-1) + 1)))
    Y_max = int(np.ceil(n_y*alpha_Y * (stride_Y*(N_y-1) + 1)))
    X_max = int(np.ceil(n_x*alpha_X * (stride_X*(N_x-1) + 1)))
    CH_max = ch*CH
    V = torch.zeros([batch_size, T_max, Y_max, X_max, CH_max], dtype=torch.float32, device=phis.device)
    print(V.shape)

    for b in range(batch_size):
        for phi in range(n_philters):
            print(phi)

            for i in range(T):
                center_T = t*alpha_T * (stride_T*i + 0.5)
                for j in range(N_y):
                    center_N_y = n_y*alpha_Y * (stride_Y*j + 0.5)
                    for k in range(N_x):
                        center_N_x = n_x*alpha_X * (stride_X*k + 0.5)
                        for l in range(CH):
                            min_CH = l*ch
                            max_CH = (l+1)*ch

                            # We can now select a philter and compute its addition to the video
                            #   after deformations and upsampling.

                            this_phi = phis[j, k, l, phi]
                            this_phi_vol = torch.zeros([T_max, Y_max, X_max, ch], dtype=torch.float32, device=phis.device)

                            this_a = a[b, i, j, k, l, phi]
                            this_mu_T = mu_T[b, i, j, k, l, phi]
                            this_sigma_T = sigma_T[b, i, j, k, l, phi]
                            this_mu_Y = mu_Y[b, i, j, k, l, phi]
                            this_mu_X = mu_X[b, i, j, k, l, phi]
                            this_sigma_Y = sigma_Y[b, i, j, k, l, phi]
                            this_sigma_X = sigma_X[b, i, j, k, l, phi]
                            this_rot = rot[b, i, j, k, l, phi]

                            for p in range(t):
                                offset_t = alpha_T*(this_sigma_T * (p+0.5 - t/2))
                                offset_mu_T = t*alpha_T*this_mu_T

                                gauss_mu_T = center_T + offset_t + offset_mu_T
                                gauss_sigma_T = alpha_T * this_sigma_T / 2
                                time_gauss = gauss(gauss_mu_T, gauss_sigma_T, this_sigma_T)

                                min_T = int(np.floor(max(0, 
                                    (gauss_mu_T.detach() - np.sqrt(6)*gauss_sigma_T.detach()).cpu().numpy())))
                                max_T = int(np.ceil(min(T_max, 
                                    (gauss_mu_T.detach() + np.sqrt(6)*gauss_sigma_T.detach()).cpu().numpy())))

                                phi_spatial_vol = torch.zeros([Y_max, X_max, ch], dtype=torch.float32, device=phis.device)

                                for q in range(n_y):
                                    offset_y = alpha_Y*(this_sigma_Y * (q+0.5 - n_y/2))
                                    offset_mu_Y = n_y*alpha_Y*this_mu_Y

                                    for r in range(n_x):
                                        offset_x = alpha_X*(this_sigma_X * (r+0.5 - n_x/2))
                                        offset_mu_X = n_x*alpha_X*this_mu_X

                                        gauss_mu_Y = center_N_y + offset_mu_Y + \
                                            torch.cos(this_rot)*offset_y + torch.sin(this_rot)*offset_x
                                        gauss_mu_X = center_N_x + offset_mu_X + \
                                            torch.cos(this_rot)*offset_x - torch.sin(this_rot)*offset_y
                                        gauss_sigma_Y = alpha_Y * this_sigma_Y / 2
                                        gauss_sigma_X = alpha_X * this_sigma_X / 2
                                        gauss_rot = this_rot
                                        spatial_gauss = gauss2d(gauss_mu_X, gauss_mu_Y,
                                                                gauss_sigma_X, gauss_sigma_Y,
                                                                gauss_rot, this_sigma_X*this_sigma_Y)

                                        min_Y = int(np.floor(max(0, 
                                            (gauss_mu_Y.detach() - \
                                                np.sqrt(6) * (torch.cos(this_rot.detach())*gauss_sigma_Y.detach() + \
                                                    torch.abs(torch.sin(this_rot.detach())*gauss_sigma_X.detach()))).cpu().numpy())))
                                        max_Y = int(np.ceil(min(Y_max, 
                                            (gauss_mu_Y.detach() + \
                                                np.sqrt(6) * (torch.cos(this_rot.detach())*gauss_sigma_Y.detach() + \
                                                    torch.abs(torch.sin(this_rot.detach())*gauss_sigma_X.detach()))).cpu().numpy())))
                                        min_X = int(np.floor(max(0, 
                                            (gauss_mu_X.detach() - \
                                                np.sqrt(6) * (torch.cos(this_rot.detach())*gauss_sigma_X.detach() + \
                                                    torch.abs(torch.sin(this_rot.detach())*gauss_sigma_Y.detach()))).cpu().numpy())))
                                        max_X = int(np.ceil(min(X_max, 
                                            (gauss_mu_X.detach() + \
                                                np.sqrt(6) * (torch.cos(this_rot.detach())*gauss_sigma_X.detach() + \
                                                    torch.abs(torch.sin(this_rot.detach())*gauss_sigma_Y.detach()))).cpu().numpy())))

                                        ys, xs = torch.meshgrid(
                                            torch.arange(min_Y, max_Y, dtype=torch.float32, device=phis.device),
                                            torch.arange(min_X, max_X, dtype=torch.float32, device=phis.device))
                                        ys = ys.reshape(-1)
                                        xs = xs.reshape(-1)

                                        phi_spatial_vol[min_Y:max_Y, min_X:max_X] += this_phi[p, q, r] * \
                                            spatial_gauss(0.5+xs, 0.5+ys).reshape(
                                                max_Y-min_Y, max_X-min_X).unsqueeze(2)

                                ts = torch.arange(min_T, max_T, dtype=torch.float32, device=phis.device)
                                this_phi_vol[min_T:max_T] \
                                    += time_gauss(0.5+ts).unsqueeze(1).unsqueeze(2).unsqueeze(3) * \
                                        phi_spatial_vol.unsqueeze(0)

                            V[b, :, :, :, min_CH:max_CH] += this_a * this_phi_vol

    ### PLOTTING ###
    if plot_save_dir is not None:
        try:
            os.system("mkdir {}".format(plot_save_dir))
        except:
            pass

        for b in range(batch_size):
            try:
                os.system("mkdir {}/{}".format(plot_save_dir, b))
            except:
                pass

            if not RGB:
                for l in range(CH_max):
                    for i in range(T_max):
                        Z = V[b, i, :, :, l].detach().cpu()

                        fig = plt.figure(figsize=(4, 8))

                        ax = fig.add_subplot(2, 1, 2)
                        ax.imshow(Z.numpy()[::-1], cmap="gray", vmin=0, vmax=1)

                        ax = fig.add_subplot(2, 1, 1, projection='3d')
                        ax.set_xlabel("x")
                        ax.set_ylabel("y")
                        ax.set_zlabel("z")
                        Y, X = torch.meshgrid(torch.arange(Y_max), torch.arange(X_max))
                        if Z.shape[0] == 1:
                            Z = Z.repeat(2, 1)
                            Y, X = torch.meshgrid(torch.arange(2*Y_max), torch.arange(X_max))
                        if Z.shape[1] == 1:
                            Z = Z.repeat(1, 2)
                            Y, X = torch.meshgrid(torch.arange(Y_max), torch.arange(2*X_max))
                        ax.plot_surface(X, Y, Z, color="black", alpha=0.2)
                        ax.set_zlim(0, 1)

                        plt.savefig("{}/{}/t_{:04d}_ch_{:04d}.png".format(plot_save_dir, b, i, l))
                        plt.close()

                    try:
                        os.system("rm {}/{}/ch_{:04d}_vid.mp4".format(plot_save_dir, b, l))
                    except:
                        pass
                    os.system("ffmpeg -r 2 -i {}/{}/t_%04d_ch_{:04d}.png {}/{}/ch_{:04d}_vid.mp4".format(
                        plot_save_dir, b, l, plot_save_dir, b, l))
            else:
                for i in range(T_max):
                    fig = plt.figure(figsize=(12, 8))

                    ax = fig.add_subplot(2, 3, 5)
                    ax.imshow(V[b, i, :, :, :3].detach().cpu().numpy()[::-1], vmin=0, vmax=1)

                    for l in range(3):
                        Z = V[b, i, :, :, l].detach().cpu()

                        ax = fig.add_subplot(2, 3, l, projection='3d')
                        ax.set_xlabel("x")
                        ax.set_ylabel("y")
                        ax.set_zlabel("z")
                        Y, X = torch.meshgrid(torch.arange(Y_max), torch.arange(X_max))
                        if Z.shape[0] == 1:
                            Z = Z.repeat(2, 1)
                            Y, X = torch.meshgrid(torch.arange(2*Y_max), torch.arange(X_max))
                        if Z.shape[1] == 1:
                            Z = Z.repeat(1, 2)
                            Y, X = torch.meshgrid(torch.arange(Y_max), torch.arange(2*X_max))
                        ax.plot_surface(X, Y, Z, color="black", alpha=0.2)
                        ax.set_zlim(0, 1)

                    plt.savefig("{}/{}/t_{:04d}.png".format(plot_save_dir, b, i))
                    plt.close()

                try:
                    os.system("rm {}/{}/vid.mp4".format(plot_save_dir, b))
                except:
                    pass
                os.system("ffmpeg -r 2 -i {}/{}/t_%04d.png {}/{}/vid.mp4".format(
                    plot_save_dir, b, plot_save_dir, b))

    return V

# phis: [[N_y, N_x], [CH], n_philters, t, n_y, n_x, ch]
    # a: [batch, T, N_y, N_x, [CH], n_philters]
    # mu_T, mu_Y, mu_X,
    # sigma_T, sigma_Y, sigma_X,
    # rot,
class HierarchicalGenerativeModel(object):
    def __init__(self,
        phis_list,
        alpha_Ts, alpha_Ys, alpha_Xs,
        stride_Ts, stride_Ys, stride_Xs,
        device,
    ):
        self.n_layers = len(phis_list)
        self.device = device

        for L in range(self.n_layers):
            phis = phis_list[L]

            if len(phis.shape) == 5:
                phis = phis.unsqueeze(0).unsqueeze(1).unsqueeze(2)
            if len(phis.shape) == 6:
                phis = phis.unsqueeze(0).unsqueeze(1)
            if len(phis.shape) == 7:
                phis = phis.unsqueeze(2)

            phis = phis / (1e-8 + torch.sum(torch.abs(phis.detach()), [4, 5, 6, 7]).unsqueeze(
                4).unsqueeze(5).unsqueeze(6).unsqueeze(7))

            phis_list[L] = phis.to(self.device)

        self.phis_list = phis_list
        self.n_philters_list = [phis.shape[3] for phis in phis_list]
        self.alpha_Ts, self.alpha_Ys, self.alpha_Xs = alpha_Ts, alpha_Ys, alpha_Xs
        self.stride_Ts, self.stride_Ys, self.stride_Xs = stride_Ts, stride_Ys, stride_Xs

    def sample(self, n_samples, T, N_y, N_x, CH):
        pass
        #TODO: sample then generate

    def generate_video(self,
        coefs_list,
        phis_list,
        plot_save_dir=None, RGB=False
    ):
        top_down_inp = None

        for L in reversed(range(self.n_layers)):
            if type(coefs_list[L]) == list:
                coefs_list[L] = torch.stack(coefs_list[L], 0)

            if top_down_inp is not None:
                coefs_list[L] = coefs_list[L] + top_down_inp

            a, mu_T, mu_Y, mu_X, sigma_T, sigma_Y, sigma_X, rot = coefs_list[L]
            if (L==0) and (plot_save_dir is not None):
                L_plot_save_dir = plot_save_dir + "/L_{:04d}".format(L)
            else:
                L_plot_save_dir = None
            out = generate_video(
                phis_list[L],
                self.alpha_Ts[L], self.alpha_Ys[L], self.alpha_Xs[L],
                self.stride_Ts[L], self.stride_Ys[L], self.stride_Xs[L],
                a, 
                mu_T,  mu_Y, mu_X,
                sigma_T, sigma_Y, sigma_X,
                rot,
                L_plot_save_dir, RGB and (L==0),
            )
            print(out.shape)
            if L >= 1:
                batch_size, out_T, out_N_y, out_N_x, out_CH = out.shape
                out = out.reshape(
                    batch_size, out_T, out_N_y, out_N_x, 
                    out_CH//(8*self.n_philters_list[L-1]), 8, self.n_philters_list[L-1]).permute(
                    5, 0, 1, 2, 3, 4, 6)
                top_down_inp = out
            else:
                return out

    def infer_coefs(self,
        video,
        phis_list,
        max_itr=50,
        rel_grad_stop_cond=0.01,
        abs_grad_stop_cond=0.01,
        lr=0.01,
        warm_start_vars_list=None,
    ):
        video = video.type(torch.float32)
        batch_size, video_T, video_N_y, video_N_x, video_CH = video.shape

        if not warm_start_vars_list:
            vars_list = []
            for L in range(self.n_layers):
                # create variables
                if L == 0:
                    next_T, next_N_y, next_N_x, next_CH = video_T, video_N_y, video_N_x, video_CH

                _, _, _, n_philters, t, n_y, n_x, ch = phis_list[L].shape
                alpha_T, alpha_Y, alpha_X = self.alpha_Ts[L], self.alpha_Ys[L], self.alpha_Xs[L]
                stride_T, stride_Y, stride_X = self.stride_Ts[L], self.stride_Ys[L], self.stride_Xs[L]

                T = int(np.ceil((next_T/(t*alpha_T) - 1) / stride_T + 1))
                N_y = int(np.ceil((next_N_y/(n_y*alpha_Y) - 1) / stride_Y + 1))
                N_x = int(np.ceil((next_N_x/(n_x*alpha_X) - 1) / stride_X + 1))
                CH = int(np.ceil(next_CH / ch))

                [a, mu_T, mu_Y, mu_X, log_sigma_T, log_sigma_Y, log_sigma_X, raw_rot] = \
                    [torch.zeros([batch_size, T, N_y, N_x, CH, n_philters],
                        dtype=torch.float32,
                        device=self.device,
                        requires_grad=True)
                    for _ in range(8)]

                vars_list.extend([a, mu_T, mu_Y, mu_X, log_sigma_T, log_sigma_Y, log_sigma_X, raw_rot])
                next_T, next_N_y, next_N_x, next_CH = T, N_y, N_x, 8*CH*n_philters

                self.coefs_optimizer = torch.optim.Adam(vars_list, lr=lr)
        else:
            vars_list = warm_start_vars_list

        detached_phis_list = [phis.detach() for phis in phis_list]

        for itr in range(max_itr):
            self.coefs_optimizer.zero_grad()
            coefs_list = self.get_coefs_list(vars_list)
            gen_video = self.generate_video(coefs_list, detached_phis_list)
            loss = 1. / batch_size * \
                (torch.sum(torch.abs(gen_video - video)) + \
                    torch.sum(torch.stack([
                        torch.sum(torch.abs(var)) for var in vars_list], 0)))
            print("coefs itr {}:".format(itr), loss)
            loss.backward()
            self.coefs_optimizer.step()

            if all([(torch.abs(var.grad) < abs_grad_stop_cond).all()
                for var in vars_list]):
                break
            if all([(torch.abs(var.grad / (1e-8 + torch.abs(var))) < rel_grad_stop_cond).all()
                for var in vars_list]):
                break

        return vars_list, self.get_coefs_list(vars_list)

    def get_coefs_list(self, vars_list):
        coefs_list = []
        for L in range(self.n_layers):
            this_vars = vars_list[8*L:8*(L+1)]
            coefs_list.append([
                this_vars[0], # a
                this_vars[1], # mu_T
                this_vars[2], # mu_Y
                this_vars[3], # mu_X
                torch.exp(this_vars[4]), # sigma_T
                torch.exp(this_vars[5]), # sigma_Y
                torch.exp(this_vars[6]), # sigma_X
                np.pi/4 * torch.tanh(this_vars[7]) # rot
            ])
        return coefs_list

    def update_phis(self,
        video,
        coefs_list,
        n_itr=1,
        lr=0.01,
        use_warm_start_optimizer=True,
    ):
        for L in range(self.n_layers):
            self.phis_list[L].requires_grad = True
            for coef in range(8):
                coefs_list[L][coef] = coefs_list[L][coef].detach()

        if not use_warm_start_optimizer:
            self.phis_optimizer = torch.optim.Adam(self.phis_list, lr=lr)

        for itr in range(n_itr):
            self.phis_optimizer.zero_grad()
            gen_video = self.generate_video(coefs_list, self.phis_list)
            loss = torch.sum(torch.abs(gen_video - video))
            print("phis itr {}:".format(itr), loss)
            loss.backward()
            self.phis_optimizer.step()

            # normalize
            for L in range(self.n_layers):
                # phis: [[N_y, N_x], [CH], n_philters, t, n_y, n_x, ch]
                self.phis_list[L] = self.phis_list[L].detach() / \
                    (1e-8 + torch.sum(torch.abs(self.phis_list[L].detach()), [4, 5, 6, 7]).unsqueeze(
                        4).unsqueeze(5).unsqueeze(6).unsqueeze(7))
